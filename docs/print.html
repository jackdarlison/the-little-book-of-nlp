<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Little Book of NLP</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">The Little Book of NLP</li><li class="chapter-item expanded "><a href="generic/generic.html"><strong aria-hidden="true">1.</strong> Generic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="generic/word-representations.html"><strong aria-hidden="true">1.1.</strong> Word Representations</a></li><li class="chapter-item expanded "><a href="generic/metrics.html"><strong aria-hidden="true">1.2.</strong> Metrics</a></li><li class="chapter-item expanded "><a href="generic/vector-similarity.html"><strong aria-hidden="true">1.3.</strong> Vector Similarity</a></li></ol></li><li class="chapter-item expanded "><a href="classical/classical.html"><strong aria-hidden="true">2.</strong> Classical</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="classical/n-grams.html"><strong aria-hidden="true">2.1.</strong> N-Grams</a></li><li class="chapter-item expanded "><a href="classical/text-processing.html"><strong aria-hidden="true">2.2.</strong> Text Processing</a></li><li class="chapter-item expanded "><a href="classical/sequence-labelling.html"><strong aria-hidden="true">2.3.</strong> Sequence Labelling</a></li><li class="chapter-item expanded "><a href="classical/grammars.html"><strong aria-hidden="true">2.4.</strong> Grammars</a></li></ol></li><li class="chapter-item expanded "><a href="neural/neural.html"><strong aria-hidden="true">3.</strong> Neural</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="neural/rnn.html"><strong aria-hidden="true">3.1.</strong> RNN</a></li><li class="chapter-item expanded "><a href="neural/transformers.html"><strong aria-hidden="true">3.2.</strong> Transformers</a></li><li class="chapter-item expanded "><a href="neural/information-extraction.html"><strong aria-hidden="true">3.3.</strong> Information Extraction</a></li><li class="chapter-item expanded "><a href="neural/machine-translation.html"><strong aria-hidden="true">3.4.</strong> Machine Translation</a></li><li class="chapter-item expanded "><a href="neural/semantic-role-labelling.html"><strong aria-hidden="true">3.5.</strong> Semantic Role Labelling</a></li><li class="chapter-item expanded "><a href="neural/word-sense-disambiguation.html"><strong aria-hidden="true">3.6.</strong> Word Sense Disambiguation</a></li><li class="chapter-item expanded "><a href="neural/question-answering.html"><strong aria-hidden="true">3.7.</strong> Question Answering</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Little Book of NLP</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/jackdarlison/the-little-book-of-nlp" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="the-little-book-of-nlp"><a class="header" href="#the-little-book-of-nlp">The Little Book of NLP</a></h1>
<p>A brief overview of the content in UoS's COMP3225 Natural Language Processing module.</p>
<p>Note: This book was written for revision purposes and may not be accurate. If there are any inaccuracies suggest an edit!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="generic-concepts"><a class="header" href="#generic-concepts">Generic Concepts</a></h1>
<p>This section covers the concepts that are widely used throughout all areas of NLP</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word-representations"><a class="header" href="#word-representations">Word Representations</a></h1>
<p>Word representations are methods of creating a numerical representation of a span of text.</p>
<h2 id="feature-sets"><a class="header" href="#feature-sets">Feature Sets</a></h2>
<p>Feature sets are handcrafted lists of features, each represented using a feature vector which are then aggregated (summed, etc.) to create a single vector for the feature set.</p>
<p>Individual features depend on the available information for the task. </p>
<p>For example, common Named Entity Recognition features include:</p>
<ul>
<li>Word shapes</li>
<li>Word features: suffixes, prefixes, capitalisation</li>
<li>POS tags</li>
<li>Word lookups (gazetteers)</li>
</ul>
<p>They are expensive to create as the choice of features are manually chosen. This also makes them difficult to adapt and tune, as it is hard to tell what features are achieving what goals</p>
<h1 id="embeddings"><a class="header" href="#embeddings">Embeddings</a></h1>
<p>Vector embeddings are created automatically, learning representations of text based on a set of training data. </p>
<p>Embeddings represent words as a point in a continuous multi-dimensional space</p>
<h2 id="sparse-embeddings"><a class="header" href="#sparse-embeddings">Sparse Embeddings</a></h2>
<p>Sparse embeddings are called as such as many of the entries in the vector will be zero. </p>
<p>There are two methods of creating sparse embeddings</p>
<h4 id="term-frequency---inverse-document-frequency-tf-idf"><a class="header" href="#term-frequency---inverse-document-frequency-tf-idf">Term Frequency - Inverse Document Frequency (TF-IDF)</a></h4>
<p>TF-IDF uses a term-document matrix, where each cell contains the count of a specific word (rows) in a specific document (columns). </p>
<p>The matrix is weighted by two factors:</p>
<ul>
<li>Term frequency:  \( \text{TF}(t, d) = \log_{10}(\text{count}(t,d)+1) \)</li>
<li>Inverse Document Frequency: \( \text{IDF}(t)=\log_{10}(\frac{N}{\text{count}_{docs}(t)}) \)</li>
</ul>
<p>Which weights each cell by the number of times it appear in the document times the inverse of the number of documents it appears in. Giving the formula for weighting each cell as:</p>
<p>\[ \text{TF-IDF}(t,d)=\text{TF}(t,d) \times \text{IDF}(t) \]</p>
<p>Both factors are used as:</p>
<ul>
<li>Term frequency doesn't discriminate</li>
<li>Inverse document frequency is useless alone, but shows which words are important to certain documents</li>
</ul>
<h4 id="positive-pointwise-mutual-information-ppmi"><a class="header" href="#positive-pointwise-mutual-information-ppmi">Positive Pointwise Mutual Information (PPMI)</a></h4>
<p>PPMI uses a term-term matrix, where each cell counts the co-occurrences of a target word (rows) and a context word (columns). Co-occurrences are defined as the number of times the context word appears within a ±N context window around the target word. </p>
<p>PPMI is a measure of how much more two words co-occur than is expected if they were independent. </p>
<p>It is calculated as:
\[ \text{PPMI}(w, c) = \text{max}( \log_{2} \frac{P(w, c)}{P(w)P(c)}, 0) \]</p>
<p>Only positive values are used as negative values are unreliable unless the corpus is massive. </p>
<p>The probabilities are calculated: (w, c) is the cell count, (w) is the row count, and (c) is the column count, all divided by the total count</p>
<h2 id="dense-embeddings"><a class="header" href="#dense-embeddings">Dense Embeddings</a></h2>
<p>Dense embeddings are much smaller vectors (usually with dimensions in the hundreds), where all values take meaningful real numbers. </p>
<p>Static embeddings learn one fixed embedding for each word.</p>
<p>Word2vec (Skip-gram with negative sampling) is an example algorithm to compute static dense embeddings. In which a self-supervised classifier is trained to classify is two words co-occur, i.e. the context word appears in a ±N window of the target word.  The weights are then used as the embeddings for words.</p>
<p>To train the weights:</p>
<ul>
<li>The classifier initialises two sets of weights randomly for each word, one for its target representation and the other for its context representation. </li>
<li>For all words, use the N context words and sample kN random words using weighted unigram frequency (i.e. counts to the power of \(\alpha\), commonly 0.75) to give more weight to rare words</li>
<li>Adjust the weights to maximise the vector similarity of the context word pairs and minimise the similarity of the negative words</li>
</ul>
<p>As two embeddings are learnt for each word it is common to either add them together or just use the target word embeddings</p>
<p>Other types of static include:</p>
<ul>
<li><em>fasttext</em>: an extension on word2vec using a sub-word model to better handle unknown words and word sparsity</li>
<li>GloVe: captures global corpus statistics from ratios of probabilities in a term-term matrix</li>
</ul>
<p>Contextual embeddings (such as BERT) capture embeddings for each word sense. </p>
<h4 id="benefits-over-sparse-embeddings"><a class="header" href="#benefits-over-sparse-embeddings">Benefits over sparse embeddings</a></h4>
<p>Dense embeddings are better than sparse as:</p>
<ul>
<li>They require less weights due to the lower dimensions, which helps with generalisation and avoiding overfitting</li>
<li>They capture relations between words</li>
</ul>
<h4 id="semantic-properties-of-dense-embeddings"><a class="header" href="#semantic-properties-of-dense-embeddings">Semantic Properties of Dense Embeddings</a></h4>
<p>In word2vec the size of the context window can alter the types of association between vectors:</p>
<ul>
<li>Smaller context windows (±2) show first-order co-occurrence (syntagmatic association). Which means the words are typically near each other. </li>
<li>Larger context windows (±5) show second-order co-occurrence (paradigmatic association). Which means the words share similar neighbours.</li>
</ul>
<p>Embeddings encode properties such as:</p>
<ul>
<li>Relational similarity/meaning: e.g. King - man + women = queen (the parallelogram model)</li>
<li>Implicit corpus bias</li>
</ul>
<p>With multiple corpuses (e.g. historical, cultural, document type, ...) analysing the differences in word embeddings can show the change in meaning and associations words may have</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="evaluation-metrics"><a class="header" href="#evaluation-metrics">Evaluation Metrics</a></h1>
<h2 id="rouge"><a class="header" href="#rouge">ROUGE</a></h2>
<p>ROUGE is used to evaluate text summarisation.</p>
<p>It evaluates a summary by the proportion on n-grams in the summary that also appear in a reference summary</p>
<p>ROUGE is recall oriented so depended on the quantity of matches not the quality of them.</p>
<h2 id="bleu"><a class="header" href="#bleu">BLEU</a></h2>
<p>BLEU is used to evaluate machine translations.</p>
<p>It evaluates a generated text against a reference text by comparing the number of n-grams that appear in the generated that also appear in the reference text</p>
<p>BLEU is purely precision based (how much of the translation is good) and ignores recall factors (how much did it translate)</p>
<h2 id="perplexity"><a class="header" href="#perplexity">Perplexity</a></h2>
<p>Perplexity is used to evaluate language models, i.e. models which assign a probability distribution for the next word in a sequence. </p>
<p>It measures how good a vocabulary is it predicting a target text. It is computed as the probability of the words in the text appearing in that order, which is then inverted and normalised by the number of words. </p>
<p>Perplexity is given as:
\[ Perplexity(w_1...w_N)=\sqrt[N]{\frac{1}{P(w_1w_2...w_N)}} \]</p>
<p>Generalising to a bigram model gives:
\[ Perplexity(w_1...w_N)=\sqrt[N]{ \prod_{i=1}^{N} \frac {1} {P(w_i \vert  w_{i-1})} } \]</p>
<p>The probability of a sequence of words using the Markov assumption (bigram model) is:</p>
<p>\[ P(w_{1:n}) = P(w_1)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<p>Where there is a start if sentence marker:</p>
<p>\[ P(w_{1:n}) = P(w_1| \lt s \gt)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<h2 id="precision-recall-and-f1"><a class="header" href="#precision-recall-and-f1">Precision, Recall, and F1</a></h2>
<p>Precision, Recall, and F1 are used to evaluate classification tasks.</p>
<p>Precision is defined as the proportion of predicted items that are actually correct compared to all items predicted to be correct:
\[ precision=\frac{TP}{TP+FP} \]</p>
<p>Recall is defined as the proportion of predicted items that are actually correct compared to the actual set of correct items
\[ recall=\frac{TP}{TP+FN} \]</p>
<p>There is a trade off between these two metrics so F1 score is used instead, which incorporates both metrics. 
\[ F1=\frac{2 \times precision \times recall}{precision+recall} \]</p>
<p>To adapt these to multi-class problems there are two methods:</p>
<ul>
<li>macro-averaging: compute the performance for each class individually, then average over the classes</li>
<li>micro-averaging: pool the decisions for each class into a single binary confusion matrix, then compute precision and recall</li>
</ul>
<h2 id="cross-entropy-loss"><a class="header" href="#cross-entropy-loss">Cross Entropy Loss</a></h2>
<p>Cross entropy loss (negative log likelihood loss) measures how close an estimated output \(\hat{\ y} \) is to the correct output \( y \). We aim to minimise the negative log probability of the true \( y \) labels in the training data. Cross entropy loss is given as:</p>
<p>\[ L_{CE}(\hat{\ y}, y) = -( y \log \hat{\ y} + (1 - y) \log (1 - \hat{\ y}) )\]</p>
<p>When the classifier is binary, correct output being 0 or 1, the cross entropy loss is just \(\ -\log\hat{\ y} \)</p>
<p>For multi-class output <em>Categorical Cross Entropy Loss</em> is given:</p>
<p>\[ L_{CCE}(\hat{\ y},y) = -[\sum_{i=1}^{N} y_i * log(\hat{y_i})] \]</p>
<p>To find the loss of a sequence of outputs, we average the cross entropy loss over all the output states:</p>
<p>\[ L = \frac{1}{T}\sum^T_{i=1}L_{CCE}(\hat{y_i}, y_i) \]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vector-similarity"><a class="header" href="#vector-similarity">Vector Similarity</a></h1>
<p>Measuring the similarity of two vectors is an important topic as many NLP applications use vector representations of words, i.e. embeddings</p>
<p>A simple method of measuring how similar two vectors are is the dot product:</p>
<p>\[ v \cdot w = \sum^N_{i=1} v_iw_i \]</p>
<p>However, this is method is biased towards longer vectors. <strong>Cosine similarity</strong> or the normalised dot product is commonly used instead:</p>
<p>\[ \text{cosine}(\theta) = \frac{a \cdot b}{|a||b|} \]</p>
<p>Where vectors are pre-normalised these metrics are interchangeable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classical-nlp"><a class="header" href="#classical-nlp">Classical NLP</a></h1>
<p>This section covers areas of NLP that exist outside of deep neural networks. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n-grams"><a class="header" href="#n-grams">N-Grams</a></h1>
<p>A language model is one that assigns a probability to each possible next word in a sequence. </p>
<p>The probability of a sequence of words is the joint probability of each word given all previous words.</p>
<p>\[ P(w_{1:n}) = P(w_1)P(w_2 \vert w_1)P(w_3 \vert w_{1:2})...P(w_n \vert w_{1:n-1}) \]</p>
<p>As there is no way to compute the probability of a word given a long sequence of preceding words, n-grams use a generalised form of the Markov assumption (the current state only depends on the previous, i.e. the bigram) where the current state depends on the previous n-1 states. </p>
<p>For a bigram model:</p>
<p>\[ P(w_{1:n}) = P(w_1)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<p>Where there is a start if sentence marker:</p>
<p>\[ P(w_{1:n}) = P(w_1| \lt s \gt)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<p>The probability (maximum likelihood estimation, MLE) for an n-gram is given by the count of the n-gram normalised by the count of the preceding (n-1)-gram:</p>
<p>\[ P(w_n | w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)}{C(w_{n-N+1:n-1})} \]</p>
<p>Where \( N \) is the size of the n-gram</p>
<p>To evaluate an n-gram model <a href="classical/../generic/metrics.html#Perplexity">perplexity</a> is used.</p>
<h2 id="dealing-with-unknown-words"><a class="header" href="#dealing-with-unknown-words">Dealing with Unknown Words</a></h2>
<p>It is possible that words may appear in the test set that where not in the training set. </p>
<p>One method of dealing with this is to enforce a <em>close vocabulary</em>, where all test words need to be known.</p>
<p>However, in most situations language models need to be able to handle unknown words, also known as <em>out of vocabulary</em> (OOV) words.  To do this a new pseudo-word token &lt;UNK&gt; is added to the vocabulary, making it an <em>open vocabulary</em></p>
<p>There are two common ways to create an open vocabulary:</p>
<ul>
<li>Choose a fixed vocabulary and replace all words in the training set with the &lt;UNK&gt; that do not appear in the fixed vocabulary</li>
<li>Replace all words in the training set that have less than a certain frequency or are not in the most frequent X words with &lt;UNK&gt;</li>
</ul>
<p>This pseudo-word is then estimated like any other word. Unknown words in the test set are then treated as the &lt;UNK&gt; token</p>
<h2 id="dealing-with-sparse-data"><a class="header" href="#dealing-with-sparse-data">Dealing with Sparse Data</a></h2>
<p>Another issue that occurs in n-gram models is the sparsity of n-grams, i.e. known words appearing in a new sequence or context.</p>
<h4 id="laplace-smoothing"><a class="header" href="#laplace-smoothing">Laplace Smoothing</a></h4>
<p>One method of solving this is shifting some of the probability mass from probable words to words that appear less. This is known as <strong>Laplace</strong> smoothing where \( k \) is added to all all word counts, this is commonly known as add-1 or add-\(k\) smoothing. </p>
<p>\[ P_{\text{Laplace}}(w_n \vert w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)+k}{C(w_{n-N+1:n-1})+kV} \]</p>
<p>Note: \( kV \) has been added to the denominator to take into account the extra counts across the whole vocabulary</p>
<h4 id="backoff-and-interpolation"><a class="header" href="#backoff-and-interpolation">Backoff and Interpolation</a></h4>
<p>Another method of dealing with sparse n-grams is to take into account the probability of the lower-order n-grams. There are two methods of doing this.</p>
<p><strong>Backoff</strong> is one method, which uses the highest-order n-gram that has been seen.</p>
<p>In order for backoff to give a valid probability distribution, a function \( \alpha \) is used to distribute the mass to the lower-order n-grams. This is known as <strong>Backoff with discounting</strong> or <strong>Katz Backoff</strong></p>
<p><strong>Interpolation</strong> is another method which mixes the probabilities of the n-gram and its lower-order variations. This is done by adding the probability of each n-gram together, each weighted by some factor \( \lambda_i \). The sum of all weights needs to add to 1 to ensure the probability distribution remains valid. </p>
<p>\[ \hat{P}(w_n | w_{n-2:n-1}) = \lambda_1P(w_n) + \lambda_2P(w_n|w_{n-1}) + \lambda_3P(w_n|w_{n-2:n-1}) \]</p>
<p>It is possible to also compute weights depending on the context sequences. </p>
<p>These weights are trained using a held-out corpus, ensuring it does not overfit to the actual training data. </p>
<p>Other forms of dealing with unknown contexts are:</p>
<ul>
<li>Stupid backoff</li>
<li>Kesner-Ney Smoothing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="text-processing"><a class="header" href="#text-processing">Text Processing</a></h1>
<h2 id="regex"><a class="header" href="#regex">Regex</a></h2>
<p>Regex is a language used to find text that matches a certain pattern.</p>
<p>Some useful pattern notations:</p>
<ul>
<li>The common operator with characters is concatenation</li>
<li>Brackets match a single character from a range of characters, e.g. [a-zA-Z] or [abcdef]</li>
<li>Carat at the start of brackets indicates &quot;not&quot; in range, e.g. [^a] is not &quot;a&quot;</li>
<li>A ? indicates the preceding symbol is optional</li>
<li>A * indicates 0 or more of the preceding symbol, + is 1 or more</li>
<li>{x, X} indicates a range between x and X of the preceding characters, variations of {x}, {x,} and {,X} exist</li>
<li>A . indicates any character</li>
<li>^ and $ indicate the start and end of lines respectively</li>
<li>\b is a word boundary</li>
<li>The | indicates the &quot;or&quot; operator</li>
<li>\s (\S) indicate (not) whitespace characters</li>
<li>\w (\W) indicate (not) word characters</li>
<li>\d (\D) indicate (not) digit characters</li>
<li>() indicate a capture group, where \1 .. \n can be used to reference the specific capture group match. </li>
<li>(?:) indicates a non-capturing group, useful with the | operator. </li>
<li>(?=) (?!) indicate positive and negative lookahead (non-matching)</li>
<li>(?&lt;=) (?&lt;!) indicate positive and negative lookbehind (non-matching)</li>
</ul>
<h2 id="text-processing-1"><a class="header" href="#text-processing-1">Text Processing</a></h2>
<p>Given a text, we may want to separate the text into individual words. This process is known as word tokenisation, other forms exist which may tokenise the text into sub-word pieces or spans of words.</p>
<p>Byte Pair Encoding is an automated method of tokenising a text into sub-words. It works as follows:</p>
<ul>
<li>Start with a set of all characters, the vocabulary.</li>
<li>Split the text into characters</li>
<li>Find the most frequent concatenation of two tokens from the vocabulary present within words in the text, merge them, and add the merged token into the vocabulary</li>
<li>repeat until \( k \) merges have been made.</li>
</ul>
<p>It may be useful to normalise words into a single form, of which there are two methods:</p>
<ul>
<li>Lemmas are the root (dictionary) form of words, e.g. sing is the lemma of sing, sang, and sung</li>
<li>Stems are the main body of the word with modifying suffixes removed</li>
</ul>
<h2 id="string-similarity"><a class="header" href="#string-similarity">String Similarity</a></h2>
<p>Many NLP applications may need to know how similar two strings are. </p>
<p>To do this minimum edit distance is used, which scores the number of edits needed to change a string to another. There are three common operations:</p>
<ul>
<li>Insertion, weighted 1</li>
<li>Deletion, weighted 1</li>
<li>Substitution, weighted 1 using Levenshtein distance, or commonly 2 </li>
</ul>
<p>This application uses dynamic programming, by recursively finding the best substring and building up to the whole string. Best substrings are scored by the lowest operation cost. </p>
<p>To create the best alignment between two strings, a table is built where each cell is the substrings score, and back pointers are used to keep track of the previous state. Then the lowest cost path back through the table is the best alignment.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sequence-labelling"><a class="header" href="#sequence-labelling">Sequence Labelling</a></h1>
<p>Sequence labelling is the task of assigning a label to each element in the input. </p>
<h2 id="parts-of-speech-pos"><a class="header" href="#parts-of-speech-pos">Parts of Speech (POS)</a></h2>
<p>POS tagging aims to assign a parts of speech tag to each word in the input which describes its grammatical function, e.g. nouns, determiners. POS tagging is a disambiguation task as one word may have more than one possible meaning, the goal is therefore to find the best sequence of tags for the situation</p>
<p>POS tags can be split into three types of class:</p>
<ul>
<li>Open classes: New words are frequently being created or borrowed, such as nouns, verbs, adjectives, adverbs, interjections</li>
<li>Closed classes: Mostly fixed vocabulary, typically function words used for structuring grammar, e.g. pronouns, adpositions, conjunctions...</li>
<li>Other: symbols and punctuation</li>
</ul>
<p>A list of POS labels is known as a Tagset. The most common of these is the Penn Treebank which contains 45 different labels. </p>
<p><img src="classical/../images/penn-treebank-tags.png" alt="Penn Treebank Tags" /></p>
<p>A good <em>baseline</em> model is to take the most common POS tag for a word, this has a surprisingly high accuracy of 92%</p>
<h4 id="hidden-markov-models-hmms"><a class="header" href="#hidden-markov-models-hmms">Hidden Markov Models (HMMs)</a></h4>
<p>A better model is to use a Hidden Markov Model (HMM), where we are trying to find the hidden states (POS tags) based on the observed emissions (words).</p>
<p>HMMs make two assumptions:</p>
<ul>
<li>Markov Assumption: The current state only depends on the previous</li>
<li>Output independence: The output observation (word) only depends on the state that produced it (the POS tag)</li>
</ul>
<p>HMMs consist of 5 components:</p>
<ul>
<li>\( Q \): a set of \( N \) states</li>
<li>\( A \): a transition probability matrix, representing the probability of moving from one state to another</li>
<li>\( O \): a sequence of \( T \) observations drawn of the vocabulary</li>
<li>\( B \): a sequence of observation (emission) probabilities, each representing the probability of an observation being generated at a state</li>
<li>\( \pi \): the initial probability distribution for the starting state, sums to 1.</li>
</ul>
<p>To do POS tagging we aim to <em>decode</em> the states of the HMM based on the observations</p>
<p>The most probable sequence of tags, using a bigram model, is defined as:</p>
<p>\[  \hat{t}_{1:n} \]</p>
<p>\[ =\underset{t_{1:n}}{\text{argmax}} \prod^n_{i=1}P(w_i|t_i)P(t_i|t_{i-1}) \]</p>
<p>Where:</p>
<ul>
<li>Emission probability \( P(w_i|t_i) \) is given as \( \frac{C(t_i, w_i)}{C(t_i)} \)</li>
<li>Transition probability \( P(t_i | t_{i-1}) \) is given as \( \frac{C(t_{i-1}, t_i)}{C(t_{i-1})} \)</li>
</ul>
<p>To calculate the most probable sequence of tags the Viterbi algorithm is used, which is an instance of dynamic programming. Viterbi works by:</p>
<ul>
<li>Filling a probability matrix, where each cell \( (t, j) \) represents the probability of that the HMM is in state \( j \) after seeing the first \( t \) observations and passing though the most probable sequence \( q_1, \dots, q_{t-1} \) (i.e. the max of all possible sequences) towards the current state. This is computed recursively, given the previous state through, transmission probability, and emission probability: \( v_t(j) = \max^N_{i=1} v_{t-1}(i)A_{ij}B_j(O_t)) \). </li>
<li>At each step keep track of the previous state</li>
<li>When all cells are computed, recursively backtrack using the pointers to find the most likely sequence of states. </li>
</ul>
<h2 id="named-entity-recognition-ner"><a class="header" href="#named-entity-recognition-ner">Named Entity Recognition (NER)</a></h2>
<p>POS tagging can determine proper nouns, however, we may want to further disambiguate them into types of entities. Named Entities are mostly all proper nouns, such as persons and locations, however may extend to times and dates.</p>
<p>NER is a crucial step towards building semantic relations, extracting events and finding relations between participants. </p>
<p>NERs may span over more than one word, meaning the task is now a span labelling problem rather than a word labelling problem. However, it is still implemented as a word-tag problem through the use of BIO tagging. </p>
<p>BIO tagging prefixes each words NER label with its position in the NER span:</p>
<ul>
<li>B: indicates the beginning of an NER span</li>
<li>I: indicates the inside/end of an NER span</li>
<li>O: indicates the word is not within an NER span</li>
</ul>
<p>An extension, BIOES, adds:</p>
<ul>
<li>E: indicates the end of a span</li>
<li>S: indicates a single word NER</li>
</ul>
<h4 id="conditional-random-fields-crfs"><a class="header" href="#conditional-random-fields-crfs">Conditional Random Fields (CRFs)</a></h4>
<p>CRFs are used because we want to use Feature sets to represent words, allowing us to deal better with unknown words, which don't work well within HMMs. </p>
<p>HMMs compute the best sequence (argmax Y of  P(Y|X)) based on Bayes rule and P(X|Y). In contrast CRF computes the sequence probability directly, by computing log-linear functions over local feature vectors which is aggregated and normalised to produce the global probability for the whole sequence. Weights are also created for each of the features, which are trained using gradient descent.</p>
<p>Linear chain CRFs are decoded using the Viterbi algorithm, like HMMs. </p>
<p><a href="classical/../generic/word-representations.html">Feature vectors</a> are a common way of embedding words for use in computations. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="grammars"><a class="header" href="#grammars">Grammars</a></h1>
<p>Grammars enable the formalisation of language and grammar. This has wide uses in NLP such as sense disambiguation, grammatical checking, etc.</p>
<h2 id="context-free-grammars-cfgs"><a class="header" href="#context-free-grammars-cfgs">Context Free Grammars (CFGs)</a></h2>
<p>CFGs grammars consist of a set of production rules, containing a non-terminal symbol and the symbols it generates, shown though an arrow.</p>
<p>CFGs split language into constituents, which is the idea that groups of words can be split into units, or constituents. </p>
<p>Applying a set of productions to a text is a derivation, which are shown through parse trees:</p>
<ul>
<li>Terminal nodes (words) are the leaves</li>
<li>Non-terminal nodes define lexical categories (e.g. POS tags)</li>
<li>The root node is the usually the start symbol (S)</li>
</ul>
<p>There can be multiple derivations of a single sentence using the same grammar, this can lead to ambiguity to the intended parse. </p>
<p>Sentences which cannot be represented using the grammar are known as ungrammatical, the set of all possible generations are the grammatical sentences. This is known as structural ambiguity. </p>
<p><strong>Treebanks</strong> are corpuses in which every sentence is annotated using a parse tree. E.g. the Penn Treebank POS tagset</p>
<h4 id="cky-parsing"><a class="header" href="#cky-parsing">CKY Parsing</a></h4>
<p>Cocke-Kasami Younger (<strong>CKY</strong>) parsing is a classic dynamic programming (chart parsing) approach to parsing.</p>
<p>For CKY grammars must be in Chomsky Normal Form, where productions are either two non-terminals or a single terminal. All CFGs can be converted into CNF through the addition of new dummy non-terminals. </p>
<p>CKY represents all possible parses of a sentence by filling a 2D matrix where each cell \((i, j)\) represents possible constituents in the fenceposted span between \(i\) and \(j\). The matrix is filled left-to-right and bottom-up, ensuring that we have the solutions to all sub problems of a specific cell. Backpointers are kept to show where the individual parses were derived </p>
<p>Filling the parse representation matrix gives us all possible parses (by follow back pointers from (0, n)) However it does not tell us which is best. </p>
<p>To choose the best parse a neural constituency parser is created. This assigns a score to each constituent then a modified CKY is used to combine the scores in a grammatically valid way</p>
<p>To evaluate parsers F1 score is used, based on the constituents being in the same starting, ending points and non-terminal symbol </p>
<h2 id="dependency-grammars"><a class="header" href="#dependency-grammars">Dependency Grammars</a></h2>
<p>Typed Dependency grammars take the form directed acyclical graphs. Where each node is a word and each arc is a label for the grammatical relation. Every word only has one incoming relation and there must be a path to every word from the root (which has no incoming)</p>
<p>This system provides useful information for tasks such as information extraction, semantic parsing and question answering.</p>
<p>Dependency grammars are much less structural than CFGs as no positional information is given for the words in a sentence, only the relations between two words are given. </p>
<p>The arcs are directed, going from the &quot;head&quot; word to the &quot;dependent&quot; word. An arc is projective if there is a path from the head to all words between the head and the dependent. The whole tree is projective if every arc is.</p>
<p><strong>Transition-based parsing</strong> is used to parse dependency grammars. It consists of:</p>
<ul>
<li>Stack: where the parse is built, initialised with the root node</li>
<li>An input buffer: the sentence (in order)</li>
<li>A set of relations: the created dependency tree</li>
<li>A set of transition operators. e.g. 
<ul>
<li>shift: move word from buffer to stack</li>
<li>left arc: create head-dependent relation between top and second, remove second</li>
<li>right arc: create head-dependent relation between second and top, remove top</li>
</ul>
</li>
</ul>
<p>At each stage:</p>
<ul>
<li>Shift a word onto the stack from the buffer</li>
<li>Examine the top two elements and choose to apply a transition operator if possible. </li>
</ul>
<p>This method can only produce projective dependency trees</p>
<p><strong>Graph-based parsing</strong> uses the maximum spanning trees to create dependency structures. </p>
<p>This works by creating a graph which is fully connected, weighted and directed where the vertices are input words and all possible relations are shown through arcs. A root node with arcs to all nodes is added.</p>
<p>The weights of each arc reflect the score for a possible head dependent relation. This can be assigned either using feature based methods or neural based methods</p>
<p>The best possible dependency parse is equivalent to the maximum spanning tree. Which is a sub-graph of the original starting from the root with one path to all nodes with the highest score among the edges. </p>
<p>Both parsing approaches are trained using supervised machine learning using data from tree banks. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="neural-nlp"><a class="header" href="#neural-nlp">Neural NLP</a></h1>
<p>This section covers deep neural networks and their applications in NLP.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recurrent-neural-networks-rnns"><a class="header" href="#recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</a></h1>
<p>Language is inherently temporal in nature, meaning we want to be able to process words in relation to words which come before it and potentially after. </p>
<p>Simple recurrent networks (Elman Networks) achieve temporal processing by introducing recurrent connections. We model each input in the sequence as happening at a time \( t \) and the recurrent connections allow us to use the output of the hidden layer at \( t-1 \) as another input to the hidden layer at \( t \). Therefore information from previous inputs is aggregated and passed into future processing. </p>
<p>The complexity of the recurrent connections is reduced by unravelling the connections through time, creating a hidden layer of individual nodes which process the input \( t \) and previous hidden layer \( t-1 \). Training is then done similar to standard neural networks, propagating the loss through the time states and back-propagating to the start, altering the weights. </p>
<p>As recurrent networks rely on processing previous hidden states to calculate the current state, they can not be parallelised making large networks slow to compute. </p>
<h2 id="long-short-term-memory-lstm-cells"><a class="header" href="#long-short-term-memory-lstm-cells">Long-Short Term Memory (LSTM) cells</a></h2>
<p>LSTM cells are a more sophisticated form of recurrent connections, designed to give RNNs to ability to learn and forget information as well as solve the limited information retention that simple recurrent networks have. </p>
<p>This is achieved by the addition of the context output, which is also passed through time like the hidden layer outputs. </p>
<p><img src="neural/../images/lstm.png" alt="LSTM" /></p>
<p>Information is added and removed from the context and hidden vectors through the use of gates, which consist of sigmoid activations and element-wise multiplication. </p>
<p>As LSTMs are modular in nature, so they can be unrolled through time like simple RNNs, just with the addition of a new input and output.</p>
<p>LSTMs (and its simpler version the Gated Recurrent Unit,  GRU) are the standard architecture for any recurrent based network</p>
<h2 id="rnn-architecture"><a class="header" href="#rnn-architecture">RNN Architecture</a></h2>
<h4 id="stacked-rnns"><a class="header" href="#stacked-rnns">Stacked RNNs</a></h4>
<p>As RNNs can output at each hidden state we can stack multiple layers of RNNs on top of each other, using the output from one layer as the input in the layer above. </p>
<p>This allows us to capture varying levels of abstraction across the levels, similar to how human perception works. Generally outperforming single layered networks</p>
<p>However, training costs rise quickly with the number of layers</p>
<h4 id="bidirectional-rnns"><a class="header" href="#bidirectional-rnns">Bidirectional RNNs</a></h4>
<p>Many applications have access to the whole input instead of just the preceding input. In these it would make sense to also use the right-to-left context to aid in predicting. In addition one directional RNNs naturally reflect more information about the end of a sentence than the beginning, a problem bidirectional solves. </p>
<p>This is achieved by training to RNNs, one processes the sequence forward and one processes it in reverse. The final hidden layer (or pooled outputs) are then combined. </p>
<p>Concatenation is the most common form of combining the two outputs, however, other methods such as element-wise addition or multiplication are acceptable. </p>
<h2 id="recurrent-network-patterns"><a class="header" href="#recurrent-network-patterns">Recurrent Network Patterns</a></h2>
<p>Recurrent networks can be assembled in multiple different ways, each searching a specific task. </p>
<h4 id="for-language-models"><a class="header" href="#for-language-models">For Language models</a></h4>
<p><img src="neural/../images/language-modelling.png" alt="Language Modelling" /></p>
<p>To use RNNs for language models, we first input a start of sequence token (and some context) where the output (softmaxed) is then the predicted next word. At the next time step the previous prediction is used as the input, where this process repeats until a sequence length is reached or a end of sequence token is generated. </p>
<p>This process of using the previous output as the next input is known as <strong>autoregressive generation</strong></p>
<p>To train, teacher forcing is used which uses the correct input instead of the previous predicted word. This stops the model drifting to far from what is expected. To adjust the weights averaged cross entropy loss over the predicted words is used. </p>
<p><em>Weight tying</em> can reduce the number of parameters by using the same embedding matrix for the input for the output weights when predicting the next word.</p>
<h4 id="for-sequence-labelling"><a class="header" href="#for-sequence-labelling">For Sequence Labelling</a></h4>
<p><img src="neural/../images/sequence-labelling.png" alt="Sequence labelling" /></p>
<p>For sequence labelling, the output of each hidden layer is used to predict the label for each input token.</p>
<h4 id="for-sequence-classification"><a class="header" href="#for-sequence-classification">For Sequence classification</a></h4>
<p><img src="neural/../images/sequence-classification.png" alt="Sequence Classification" /></p>
<p>For sequence classification, all but the last hidden layers are ignored. The last hidden layer is fed into a separate MLP+softmax classifier. </p>
<p>The cross entropy loss from the classifier is used to train the whole network, known as end-to-end training</p>
<p>Instead of using the last hidden state we can use a pooling function to aggregate all the hidden states such as:</p>
<ul>
<li>element-wise mean</li>
<li>element-wise max</li>
<li>element-wise addition</li>
</ul>
<h4 id="for-sequence-to-sequence"><a class="header" href="#for-sequence-to-sequence">For Sequence-to-Sequence</a></h4>
<p><img src="neural/../images/encoder-decoder.png" alt="Encoder-Decoder" /></p>
<p>Sequence to sequence (lengths are each may be different) problems such as machine translation combine two RNNs. The first network, the <strong>encoder</strong>, encodes the input sequence into a single vector known as the context vector, which represents all the information the second RNN needs to know about the first. The second network, the <strong>decoder</strong>, combines the context vector to generate the output sequence</p>
<p>The <em>context vector</em> can be created either by using the final hidden state or by pooling (max, mean, add, ...) all the hidden states of the encoder. </p>
<p>In addition the context vector is usually given to all states of the decoder directly, to ensure that its influence remains present thought out the entire decoder sequence. </p>
<p>Training the network is done end-to-end, using the averaged cross entropy loss of the decoder outputs to adjust the weights. Teacher forcing is also used in the decoder. </p>
<h2 id="attention"><a class="header" href="#attention">Attention</a></h2>
<p>In encoder-decoder models the context vector acts as a bottleneck, as it needs to represent everything there is to know about the input sequence in a single vector. </p>
<p>Attention solves this problem by taking a weighted sum over the hidden states of the encoder as each time step in the decoder. </p>
<p>Attention follows a three step process:</p>
<ul>
<li>Compute the relevance (score) of the previous decoder hidden state and each of the encoder hidden states.</li>
<li>Softmax the scores to create a probability distribution</li>
<li>Calculate the context vector for the current decoder state by using the distribution to take a weighted average over all the encoder hidden states. </li>
</ul>
<p>The most simple scoring function is the dot product between the two vectors (known as dot product attention), however it is also possible to use its own set of weights to score the two vectors which are trained with the rest of the model</p>
<p><strong>Cross attention</strong> is a more sophisticated form of attention, that is similar in nature to multi head <a href="neural/transformers.html">Self Attention</a>, but the queries come from the decoder and the keys and values come from the encoder. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers"><a class="header" href="#transformers">Transformers</a></h1>
<p>LSTMs and GRUs can handle long distance information, however, as they are recurrent in nature they cannot be parallelised, making them scale poorly with long sequence. </p>
<p>Transformers are an architecture that can handle both long distance information and be parallelised, making them the best model for language processing.</p>
<p>In a transformer each input has access to all previous inputs, known as causal transformers (variations exist using different combinations of inputs), allowing the model to handle distance information. Most importantly, calculations performed for the current input input are independent of other inputs, making the system able to be parallelised. </p>
<p>The core idea behind using other inputs to calculate the current input is the <strong>self-attention</strong>, which compares the current input to other inputs revealing their relevance in the current context. </p>
<p>Transformers are contained in a modular block, taking an input sequence and outputting a sequence of the same length allowing them to be stacked. The transformer block consists of 4 layers:</p>
<ol>
<li>Self-attention layer</li>
<li>Layer normalise</li>
<li>Feedforward layer</li>
<li>Layer normalise</li>
</ol>
<p>Each layer feeds into the next layer. Residual connections are also added around the self-attention and feedforward layers (input is added to the output vector), which allows higher layers direct access to the information from lower layers. The normalisation keeps the values of the layer outputs and residual connections in a range that facilitates gradient based learning. </p>
<p>Layer normalisation is implemented by subtracting the mean and dividing by the standard deviation, making a distribution with a mean of 0 and a standard deviation of 1. Gain and offset parameters are added after</p>
<h2 id="self-attention"><a class="header" href="#self-attention">Self-attention</a></h2>
<p>Self-attention is a more sophisticated variation of the attention used within encoder-decoder based models</p>
<p>In self-attention each input will play three different roles during the self-attention process:</p>
<ul>
<li><strong>Query</strong>: the current focus (i.e. input) of attention being compared to the other inputs</li>
<li><strong>Key</strong>: an input being compared to the current focus</li>
<li><strong>Value</strong>: the value used to compute the output for the current focus of attention</li>
</ul>
<p>To represent the roles a weight matrix will be created for each, which will be used to project the input \( x_i \) onto its representation of each role. \[q_i = W^Qx_i;\ k_i = W^Kx_i;\ v_i = W^Vx_i \]</p>
<p>Similar to encoder-decoder attention we need to calculate a relevance score for the current focus input \( x_i \) and each context input \( x_j \). However the scores are normalised by the dimensionality of the key (and query) vectors to avoid numerical issues</p>
<p>\[ score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}} \]</p>
<p>This is then softmax normalised and multiplied by \( v_i \)</p>
<p>Although, a each output can be computed independently there is no need to process each pair of inputs individually. We can instead represent the input as a matrix, and create matrices of the query, key, and value representations. This allows us to use efficient matrix multiplication implementations instead! </p>
<p>\[ Q = XW^Q;\ K=XW^K;\ V=XW^V \]</p>
<p>Given these matrices we can neatly represent the process of self attention as a single function:</p>
<p>\[ \text{SelfAttention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \]</p>
<p>As all inputs are being used as keys for all queries, if only preceding inputs should be used then the top-right triangle of the self-attention matrix should be set to negative infinity (which will become 0 with softmax)</p>
<h4 id="multi-head-attention"><a class="header" href="#multi-head-attention">Multi-head Attention</a></h4>
<p>Different words in a sentence can relate to each other is in different ways simultaneously; it is difficult for a single self-attention layer to capture all these meanings. Multi-head attention solves this by using multiple parallel self-attentions layers at the same depth.</p>
<p>Each self attention layer is given its own set of query, key, and value matrices and undergoes self-attention in the same way. The output of each self-attention is then concatenated together and projected down to the original input dimension using another set of weights \( W^O \)</p>
<h4 id="positional-embeddings"><a class="header" href="#positional-embeddings">Positional Embeddings</a></h4>
<p>So far transformers have had no notion of where each input is within the input sequence, unlike RNNs where this is built into the recurrent structure of the network. </p>
<p>To achieve positional information we create a positional embedding for each word. This is achieved through  static functions which map integers to real values vectors such they retain positional relationships, the original transformer paper used a mixture of sine and cosine functions. This positional vectors share the same dimensions as the word embeddings and they are combined through addition. </p>
<h4 id="transformers-as-language-models"><a class="header" href="#transformers-as-language-models">Transformers as Language Models</a></h4>
<p>As autoregressive generation requires the calculation of the previous input, using transformers to generate text is not able to be made parallel. </p>
<p>It is still possible to train the transformers in parallel as using teacher forcing allows all calculations to be independent. We can then use averaged cross entropy loss over the predicted outputs to adjust the weights. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="information-extraction"><a class="header" href="#information-extraction">Information Extraction</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="machine-translation"><a class="header" href="#machine-translation">Machine Translation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="semantic-role-labelling"><a class="header" href="#semantic-role-labelling">Semantic Role Labelling</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word-sense-disambiguation"><a class="header" href="#word-sense-disambiguation">Word Sense Disambiguation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="question-answering"><a class="header" href="#question-answering">Question Answering</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
