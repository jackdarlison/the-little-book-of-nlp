<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The Little Book of NLP</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">The Little Book of NLP</li><li class="chapter-item expanded "><a href="generic/generic.html"><strong aria-hidden="true">1.</strong> Generic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="generic/word-representations.html"><strong aria-hidden="true">1.1.</strong> Word Representations</a></li><li class="chapter-item expanded "><a href="generic/metrics.html"><strong aria-hidden="true">1.2.</strong> Metrics</a></li><li class="chapter-item expanded "><a href="generic/vector-similarity.html"><strong aria-hidden="true">1.3.</strong> Vector Similarity</a></li></ol></li><li class="chapter-item expanded "><a href="classical/classical.html"><strong aria-hidden="true">2.</strong> Classical</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="classical/n-grams.html"><strong aria-hidden="true">2.1.</strong> N-Grams</a></li><li class="chapter-item expanded "><a href="classical/text-processing.html"><strong aria-hidden="true">2.2.</strong> Text Processing</a></li><li class="chapter-item expanded "><a href="classical/sequence-labelling.html"><strong aria-hidden="true">2.3.</strong> Sequence Labelling</a></li><li class="chapter-item expanded "><a href="classical/grammars.html"><strong aria-hidden="true">2.4.</strong> Grammars</a></li></ol></li><li class="chapter-item expanded "><a href="neural/neural.html"><strong aria-hidden="true">3.</strong> Neural</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="neural/rnn.html"><strong aria-hidden="true">3.1.</strong> RNN</a></li><li class="chapter-item expanded "><a href="neural/transformers.html"><strong aria-hidden="true">3.2.</strong> Transformers</a></li><li class="chapter-item expanded "><a href="neural/information-extraction.html"><strong aria-hidden="true">3.3.</strong> Information Extraction</a></li><li class="chapter-item expanded "><a href="neural/machine-translation.html"><strong aria-hidden="true">3.4.</strong> Machine Translation</a></li><li class="chapter-item expanded "><a href="neural/semantic-role-labelling.html"><strong aria-hidden="true">3.5.</strong> Semantic Role Labelling</a></li><li class="chapter-item expanded "><a href="neural/word-sense-disambiguation.html"><strong aria-hidden="true">3.6.</strong> Word Sense Disambiguation</a></li><li class="chapter-item expanded "><a href="neural/question-answering.html"><strong aria-hidden="true">3.7.</strong> Question Answering</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Little Book of NLP</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/jackdarlison/the-little-book-of-nlp" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="the-little-book-of-nlp"><a class="header" href="#the-little-book-of-nlp">The Little Book of NLP</a></h1>
<p>A brief overview of the content in UoS's COMP3225 Natural Language Processing module.</p>
<p>Note: This book was written for revision purposes and may not be accurate. If there are any inaccuracies suggest an edit!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="generic-concepts"><a class="header" href="#generic-concepts">Generic Concepts</a></h1>
<p>This section covers the concepts that are widely used throughout all areas of NLP</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word-representations"><a class="header" href="#word-representations">Word Representations</a></h1>
<p>Word representations are methods of creating a numerical representation of a span of text.</p>
<h2 id="feature-sets"><a class="header" href="#feature-sets">Feature Sets</a></h2>
<p>Feature sets are handcrafted lists of features, each represented using a feature vector which are then aggregated (summed, etc.) to create a single vector for the feature set.</p>
<p>Individual features depend on the available information for the task. </p>
<p>For example, common Named Entity Recognition features include:</p>
<ul>
<li>Word shapes</li>
<li>Word features: suffixes, prefixes, capitalisation</li>
<li>POS tags</li>
<li>Word lookups (gazetteers)</li>
</ul>
<p>They are expensive to create as the choice of features are manually chosen. This also makes them difficult to adapt and tune, as it is hard to tell what features are achieving what goals</p>
<h1 id="embeddings"><a class="header" href="#embeddings">Embeddings</a></h1>
<p>Vector embeddings are created automatically, learning representations of text based on a set of training data. </p>
<p>Embeddings represent words as a point in a continuous multi-dimensional space</p>
<h2 id="sparse-embeddings"><a class="header" href="#sparse-embeddings">Sparse Embeddings</a></h2>
<p>Sparse embeddings are called as such as many of the entries in the vector will be zero. </p>
<p>There are two methods of creating sparse embeddings</p>
<h4 id="term-frequency---inverse-document-frequency-tf-idf"><a class="header" href="#term-frequency---inverse-document-frequency-tf-idf">Term Frequency - Inverse Document Frequency (TF-IDF)</a></h4>
<p>TF-IDF uses a term-document matrix, where each cell contains the count of a specific word (rows) in a specific document (columns). </p>
<p>The matrix is weighted by two factors:</p>
<ul>
<li>Term frequency:  \( \text{TF}(t, d) = \log_{10}(\text{count}(t,d)+1) \)</li>
<li>Inverse Document Frequency: \( \text{IDF}(t)=\log_{10}(\frac{N}{\text{count}_{docs}(t)}) \)</li>
</ul>
<p>Which weights each cell by the number of times it appear in the document times the inverse of the number of documents it appears in. Giving the formula for weighting each cell as:</p>
<p>\[ \text{TF-IDF}(t,d)=\text{TF}(t,d) \times \text{IDF}(t) \]</p>
<p>Both factors are used as:</p>
<ul>
<li>Term frequency doesn't discriminate</li>
<li>Inverse document frequency is useless alone, but shows which words are important to certain documents</li>
</ul>
<h4 id="positive-pointwise-mutual-information-ppmi"><a class="header" href="#positive-pointwise-mutual-information-ppmi">Positive Pointwise Mutual Information (PPMI)</a></h4>
<p>PPMI uses a term-term matrix, where each cell counts the co-occurrences of a target word (rows) and a context word (columns). Co-occurrences are defined as the number of times the context word appears within a ±N context window around the target word. </p>
<p>PPMI is a measure of how much more two words co-occur than is expected if they were independent. </p>
<p>It is calculated as:
\[ \text{PPMI}(w, c) = \text{max}( \log_{2} \frac{P(w, c)}{P(w)P(c)}, 0) \]</p>
<p>Only positive values are used as negative values are unreliable unless the corpus is massive. </p>
<p>The probabilities are calculated: (w, c) is the cell count, (w) is the row count, and (c) is the column count, all divided by the total count</p>
<h2 id="dense-embeddings"><a class="header" href="#dense-embeddings">Dense Embeddings</a></h2>
<p>Dense embeddings are much smaller vectors (usually with dimensions in the hundreds), where all values take meaningful real numbers. </p>
<p>Static embeddings learn one fixed embedding for each word.</p>
<p>Word2vec (Skip-gram with negative sampling) is an example algorithm to compute static dense embeddings. In which a self-supervised classifier is trained to classify is two words co-occur, i.e. the context word appears in a ±N window of the target word.  The weights are then used as the embeddings for words.</p>
<p>To train the weights:</p>
<ul>
<li>The classifier initialises two sets of weights randomly for each word, one for its target representation and the other for its context representation. </li>
<li>For all words, use the N context words and sample kN random words using weighted unigram frequency (i.e. counts to the power of \(\alpha\), commonly 0.75) to give more weight to rare words</li>
<li>Adjust the weights to maximise the vector similarity of the context word pairs and minimise the similarity of the negative words</li>
</ul>
<p>As two embeddings are learnt for each word it is common to either add them together or just use the target word embeddings</p>
<p>Other types of static include:</p>
<ul>
<li><em>fasttext</em>: an extension on word2vec using a sub-word model to better handle unknown words and word sparsity</li>
<li>GloVe: captures global corpus statistics from ratios of probabilities in a term-term matrix</li>
</ul>
<p>Contextual embeddings (such as BERT) capture embeddings for each word sense. </p>
<h4 id="benefits-over-sparse-embeddings"><a class="header" href="#benefits-over-sparse-embeddings">Benefits over sparse embeddings</a></h4>
<p>Dense embeddings are better than sparse as:</p>
<ul>
<li>They require less weights due to the lower dimensions, which helps with generalisation and avoiding overfitting</li>
<li>They capture relations between words</li>
</ul>
<h4 id="semantic-properties-of-dense-embeddings"><a class="header" href="#semantic-properties-of-dense-embeddings">Semantic Properties of Dense Embeddings</a></h4>
<p>In word2vec the size of the context window can alter the types of association between vectors:</p>
<ul>
<li>Smaller context windows (±2) show first-order co-occurrence (syntagmatic association). Which means the words are typically near each other. </li>
<li>Larger context windows (±5) show second-order co-occurrence (paradigmatic association). Which means the words share similar neighbours.</li>
</ul>
<p>Embeddings encode properties such as:</p>
<ul>
<li>Relational similarity/meaning: e.g. King - man + women = queen (the parallelogram model)</li>
<li>Implicit corpus bias</li>
</ul>
<p>With multiple corpuses (e.g. historical, cultural, document type, ...) analysing the differences in word embeddings can show the change in meaning and associations words may have</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="evaluation-metrics"><a class="header" href="#evaluation-metrics">Evaluation Metrics</a></h1>
<h2 id="rouge"><a class="header" href="#rouge">ROUGE</a></h2>
<p>ROUGE is used to evaluate text summarisation.</p>
<p>It evaluates a summary by the proportion on n-grams in the summary that also appear in a reference summary</p>
<p>ROUGE is recall oriented so depended on the quantity of matches not the quality of them.</p>
<h2 id="bleu"><a class="header" href="#bleu">BLEU</a></h2>
<p>BLEU is used to evaluate machine translations.</p>
<p>It evaluates a generated text against a reference text by comparing the number of n-grams that appear in the generated that also appear in the reference text</p>
<p>BLEU is purely precision based (how much of the translation is good) and ignores recall factors (how much did it translate)</p>
<h2 id="perplexity"><a class="header" href="#perplexity">Perplexity</a></h2>
<p>Perplexity is used to evaluate language models, i.e. models which assign a probability distribution for the next word in a sequence. </p>
<p>It measures how good a vocabulary is it predicting a target text. It is computed as the probability of the words in the text appearing in that order, which is then inverted and normalised by the number of words. </p>
<p>Perplexity is given as:
\[ Perplexity(w_1...w_N)=\sqrt[N]{\frac{1}{P(w_1w_2...w_N)}} \]</p>
<p>Generalising to a bigram model gives:
\[ Perplexity(w_1...w_N)=\sqrt[N]{ \prod_{i=1}^{N} \frac {1} {P(w_i \vert  w_{i-1})} } \]</p>
<p>The probability of a sequence of words using the Markov assumption (bigram model) is:</p>
<p>\[ P(w_{1:n}) = P(w_1)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<p>Where there is a start if sentence marker:</p>
<p>\[ P(w_{1:n}) = P(w_1| \lt s \gt)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<h2 id="precision-recall-and-f1"><a class="header" href="#precision-recall-and-f1">Precision, Recall, and F1</a></h2>
<p>Precision, Recall, and F1 are used to evaluate classification tasks.</p>
<p>Precision is defined as the proportion of predicted items that are actually correct compared to all items predicted to be correct:
\[ precision=\frac{TP}{TP+FP} \]</p>
<p>Recall is defined as the proportion of predicted items that are actually correct compared to the actual set of correct items
\[ recall=\frac{TP}{TP+FN} \]</p>
<p>There is a trade off between these two metrics so F1 score is used instead, which incorporates both metrics. 
\[ F1=\frac{2 \times precision \times recall}{precision+recall} \]</p>
<p>To adapt these to multi-class problems there are two methods:</p>
<ul>
<li>macro-averaging: compute the performance for each class individually, then average over the classes</li>
<li>micro-averaging: pool the decisions for each class into a single binary confusion matrix, then compute precision and recall</li>
</ul>
<h2 id="cross-entropy-loss"><a class="header" href="#cross-entropy-loss">Cross Entropy Loss</a></h2>
<p>Cross entropy loss (negative log likelihood loss) measures how close an estimated output \(\hat{\ y} \) is to the correct output \( y \). We aim to minimise the negative log probability of the true \( y \) labels in the training data. Cross entropy loss is given as:</p>
<p>\[ L_{CE}(\hat{\ y}, y) = -( y \log \hat{\ y} + (1 - y) \log (1 - \hat{\ y}) )\]</p>
<p>When the classifier is binary, correct output being 0 or 1, the cross entropy loss is just \(\ -\log\hat{\ y} \)</p>
<p>For multi-class output <em>Categorical Cross Entropy Loss</em> is given:</p>
<p>\[ L_{CCE}(\hat{\ y},y) = -[\sum_{i=1}^{N} y_i * log(\hat{y_i})] \]</p>
<p>To find the loss of a sequence of outputs, we average the cross entropy loss over all the output states:</p>
<p>\[ L = \frac{1}{T}\sum^T_{i=1}L_{CCE}(\hat{y_i}, y_i) \]</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vector-similarity"><a class="header" href="#vector-similarity">Vector Similarity</a></h1>
<p>Measuring the similarity of two vectors is an important topic as many NLP applications use vector representations of words, i.e. embeddings</p>
<p>A simple method of measuring how similar two vectors are is the dot product:</p>
<p>\[ v \cdot w = \sum^N_{i=1} v_iw_i \]</p>
<p>However, this is method is biased towards longer vectors. <strong>Cosine similarity</strong> or the normalised dot product is commonly used instead:</p>
<p>\[ \text{cosine}(\theta) = \frac{a \cdot b}{|a||b|} \]</p>
<p>Where vectors are pre-normalised these metrics are interchangeable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="classical-nlp"><a class="header" href="#classical-nlp">Classical NLP</a></h1>
<p>This section covers areas of NLP that exist outside of deep neural networks. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n-grams"><a class="header" href="#n-grams">N-Grams</a></h1>
<p>A language model is one that assigns a probability to each possible next word in a sequence. </p>
<p>The probability of a sequence of words is the joint probability of each word given all previous words.</p>
<p>\[ P(w_{1:n}) = P(w_1)P(w_2 \vert w_1)P(w_3 \vert w_{1:2})...P(w_n \vert w_{1:n-1}) \]</p>
<p>As there is no way to compute the probability of a word given a long sequence of preceding words, n-grams use a generalised form of the Markov assumption (the current state only depends on the previous, i.e. the bigram) where the current state depends on the previous n-1 states. </p>
<p>For a bigram model:</p>
<p>\[ P(w_{1:n}) = P(w_1)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<p>Where there is a start if sentence marker:</p>
<p>\[ P(w_{1:n}) = P(w_1| \lt s \gt)P(w_2 \vert w_1) \dots P(w_n \vert w_{n-1}) \]</p>
<p>The probability (maximum likelihood estimation, MLE) for an n-gram is given by the count of the n-gram normalised by the count of the preceding (n-1)-gram:</p>
<p>\[ P(w_n | w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)}{C(w_{n-N+1:n-1})} \]</p>
<p>Where \( N \) is the size of the n-gram</p>
<p>To evaluate an n-gram model <a href="classical/../generic/metrics.html#Perplexity">perplexity</a> is used.</p>
<h2 id="dealing-with-unknown-words"><a class="header" href="#dealing-with-unknown-words">Dealing with Unknown Words</a></h2>
<p>It is possible that words may appear in the test set that where not in the training set. </p>
<p>One method of dealing with this is to enforce a <em>close vocabulary</em>, where all test words need to be known.</p>
<p>However, in most situations language models need to be able to handle unknown words, also known as <em>out of vocabulary</em> (OOV) words.  To do this a new pseudo-word token &lt;UNK&gt; is added to the vocabulary, making it an <em>open vocabulary</em></p>
<p>There are two common ways to create an open vocabulary:</p>
<ul>
<li>Choose a fixed vocabulary and replace all words in the training set with the &lt;UNK&gt; that do not appear in the fixed vocabulary</li>
<li>Replace all words in the training set that have less than a certain frequency or are not in the most frequent X words with &lt;UNK&gt;</li>
</ul>
<p>This pseudo-word is then estimated like any other word. Unknown words in the test set are then treated as the &lt;UNK&gt; token</p>
<h2 id="dealing-with-sparse-data"><a class="header" href="#dealing-with-sparse-data">Dealing with Sparse Data</a></h2>
<p>Another issue that occurs in n-gram models is the sparsity of n-grams, i.e. known words appearing in a new sequence or context.</p>
<h4 id="laplace-smoothing"><a class="header" href="#laplace-smoothing">Laplace Smoothing</a></h4>
<p>One method of solving this is shifting some of the probability mass from probable words to words that appear less. This is known as <strong>Laplace</strong> smoothing where \( k \) is added to all all word counts, this is commonly known as add-1 or add-\(k\) smoothing. </p>
<p>\[ P_{\text{Laplace}}(w_n \vert w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)+k}{C(w_{n-N+1:n-1})+kV} \]</p>
<p>Note: \( kV \) has been added to the denominator to take into account the extra counts across the whole vocabulary</p>
<h4 id="backoff-and-interpolation"><a class="header" href="#backoff-and-interpolation">Backoff and Interpolation</a></h4>
<p>Another method of dealing with sparse n-grams is to take into account the probability of the lower-order n-grams. There are two methods of doing this.</p>
<p><strong>Backoff</strong> is one method, which uses the highest-order n-gram that has been seen.</p>
<p>In order for backoff to give a valid probability distribution, a function \( \alpha \) is used to distribute the mass to the lower-order n-grams. This is known as <strong>Backoff with discounting</strong> or <strong>Katz Backoff</strong></p>
<p><strong>Interpolation</strong> is another method which mixes the probabilities of the n-gram and its lower-order variations. This is done by adding the probability of each n-gram together, each weighted by some factor \( \lambda_i \). The sum of all weights needs to add to 1 to ensure the probability distribution remains valid. </p>
<p>\[ \hat{P}(w_n | w_{n-2:n-1}) = \lambda_1P(w_n) + \lambda_2P(w_n|w_{n-1}) + \lambda_3P(w_n|w_{n-2:n-1}) \]</p>
<p>It is possible to also compute weights depending on the context sequences. </p>
<p>These weights are trained using a held-out corpus, ensuring it does not overfit to the actual training data. </p>
<p>Other forms of dealing with unknown contexts are:</p>
<ul>
<li>Stupid backoff</li>
<li>Kesner-Ney Smoothing</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="text-processing"><a class="header" href="#text-processing">Text Processing</a></h1>
<p>TBC:  Regex, tokenisation, text similarity</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sequence-labelling"><a class="header" href="#sequence-labelling">Sequence Labelling</a></h1>
<p>Sequence labelling is the task of assigning a label to each element in the input. </p>
<h2 id="parts-of-speech-pos"><a class="header" href="#parts-of-speech-pos">Parts of Speech (POS)</a></h2>
<p>POS tagging aims to assign a parts of speech tag to each word in the input which describes its grammatical function, e.g. nouns, determiners. POS tagging is a disambiguation task as one word may have more than one possible meaning, the goal is therefore to find the best sequence of tags for the situation</p>
<p>POS tags can be split into three types of class:</p>
<ul>
<li>Open classes: New words are frequently being created or borrowed, such as nouns, verbs, adjectives, adverbs, interjections</li>
<li>Closed classes: Mostly fixed vocabulary, typically function words used for structuring grammar, e.g. pronouns, adpositions, conjunctions...</li>
<li>Other: symbols and punctuation</li>
</ul>
<p>A list of POS labels is known as a Tagset. The most common of these is the Penn Treebank which contains 45 different labels. </p>
<p><img src="classical/../images/penn-treebank-tags.png" alt="Penn Treebank Tags" /></p>
<p>A good <em>baseline</em> model is to take the most common POS tag for a word, this has a surprisingly high accuracy of 92%</p>
<h4 id="hidden-markov-models-hmms"><a class="header" href="#hidden-markov-models-hmms">Hidden Markov Models (HMMs)</a></h4>
<p>A better model is to use a Hidden Markov Model (HMM), where we are trying to find the hidden states (POS tags) based on the observed emissions (words).</p>
<p>HMMs make two assumptions:</p>
<ul>
<li>Markov Assumption: The current state only depends on the previous</li>
<li>Output independence: The output observation (word) only depends on the state that produced it (the POS tag)</li>
</ul>
<p>HMMs consist of 5 components:</p>
<ul>
<li>\( Q \): a set of \( N \) states</li>
<li>\( A \): a transition probability matrix, representing the probability of moving from one state to another</li>
<li>\( O \): a sequence of \( T \) observations drawn of the vocabulary</li>
<li>\( B \): a sequence of observation (emission) probabilities, each representing the probability of an observation being generated at a state</li>
<li>\( \pi \): the initial probability distribution for the starting state, sums to 1.</li>
</ul>
<p>To do POS tagging we aim to <em>decode</em> the states of the HMM based on the observations</p>
<p>The most probable sequence of tags, using a bigram model, is defined as:</p>
<p>\[  \hat{t}_{1:n} \]</p>
<p>\[ =\underset{t_{1:n}}{\text{argmax}} \prod^n_{i=1}P(w_i|t_i)P(t_i|t_{i-1}) \]</p>
<p>Where:</p>
<ul>
<li>Emission probability \( P(w_i|t_i) \) is given as \( \frac{C(t_i, w_i)}{C(t_i)} \)</li>
<li>Transition probability \( P(t_i | t_{i-1}) \) is given as \( \frac{C(t_{i-1}, t_i)}{C(t_{i-1})} \)</li>
</ul>
<p>To calculate the most probable sequence of tags the Viterbi algorithm is used, which is an instance of dynamic programming. Viterbi works by:</p>
<ul>
<li>Filling a probability matrix, where each cell \( (t, j) \) represents the probability of that the HMM is in state \( j \) after seeing the first \( t \) observations and passing though the most probable sequence \( q_1, \dots, q_{t-1} \) (i.e. the max of all possible sequences) towards the current state. This is computed recursively, given the previous state through, transmission probability, and emission probability: \( v_t(j) = \max^N_{i=1} v_{t-1}(i)A_{ij}B_j(O_t)) \). </li>
<li>At each step keep track of the previous state</li>
<li>When all cells are computed, recursively backtrack using the pointers to find the most likely sequence of states. </li>
</ul>
<h2 id="named-entity-recognition-ner"><a class="header" href="#named-entity-recognition-ner">Named Entity Recognition (NER)</a></h2>
<p>POS tagging can determine proper nouns, however, we may want to further disambiguate them into types of entities. Named Entities are mostly all proper nouns, such as persons and locations, however may extend to times and dates.</p>
<p>NER is a crucial step towards building semantic relations, extracting events and finding relations between participants. </p>
<p>NERs may span over more than one word, meaning the task is now a span labelling problem rather than a word labelling problem. However, it is still implemented as a word-tag problem through the use of BIO tagging. </p>
<p>BIO tagging prefixes each words NER label with its position in the NER span:</p>
<ul>
<li>B: indicates the beginning of an NER span</li>
<li>I: indicates the inside/end of an NER span</li>
<li>O: indicates the word is not within an NER span</li>
</ul>
<p>An extension, BIOES, adds:</p>
<ul>
<li>E: indicates the end of a span</li>
<li>S: indicates a single word NER</li>
</ul>
<h4 id="conditional-random-fields-crfs"><a class="header" href="#conditional-random-fields-crfs">Conditional Random Fields (CRFs)</a></h4>
<p>CRFs are used because we want to use Feature sets to represent words, allowing us to deal better with unknown words, which don't work well within HMMs. </p>
<p>HMMs compute the best sequence (argmax Y of  P(Y|X)) based on Bayes rule and P(X|Y). In contrast CRF computes the sequence probability directly, by computing log-linear functions over local feature vectors which is aggregated and normalised to produce the global probability for the whole sequence. Weights are also created for each of the features, which are trained using gradient descent.</p>
<p>Linear chain CRFs are decoded using the Viterbi algorithm, like HMMs. </p>
<p><a href="classical/../generic/word-representations.html">Feature vectors</a> are a common way of embedding words for use in computations. </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="grammars"><a class="header" href="#grammars">Grammars</a></h1>
<p>TBC: constituency grammars, CKY parsing, dependency grammars</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="neural-nlp"><a class="header" href="#neural-nlp">Neural NLP</a></h1>
<p>This section covers deep neural networks and their applications in NLP.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recurrent-neural-networks-rnns"><a class="header" href="#recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</a></h1>
<p>TBC: RNNs, cross-attention, applications</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformers"><a class="header" href="#transformers">Transformers</a></h1>
<p>TBC: transformers, self-attention.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="information-extraction"><a class="header" href="#information-extraction">Information Extraction</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="machine-translation"><a class="header" href="#machine-translation">Machine Translation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="semantic-role-labelling"><a class="header" href="#semantic-role-labelling">Semantic Role Labelling</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="word-sense-disambiguation"><a class="header" href="#word-sense-disambiguation">Word Sense Disambiguation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="question-answering"><a class="header" href="#question-answering">Question Answering</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>
