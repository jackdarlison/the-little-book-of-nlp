<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>RNN - The Little Book of NLP</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">The Little Book of NLP</li><li class="chapter-item expanded "><a href="../generic/generic.html"><strong aria-hidden="true">1.</strong> Generic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../generic/word-representations.html"><strong aria-hidden="true">1.1.</strong> Word Representations</a></li><li class="chapter-item expanded "><a href="../generic/metrics.html"><strong aria-hidden="true">1.2.</strong> Metrics</a></li><li class="chapter-item expanded "><a href="../generic/vector-similarity.html"><strong aria-hidden="true">1.3.</strong> Vector Similarity</a></li></ol></li><li class="chapter-item expanded "><a href="../classical/classical.html"><strong aria-hidden="true">2.</strong> Classical</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../classical/n-grams.html"><strong aria-hidden="true">2.1.</strong> N-Grams</a></li><li class="chapter-item expanded "><a href="../classical/text-processing.html"><strong aria-hidden="true">2.2.</strong> Text Processing</a></li><li class="chapter-item expanded "><a href="../classical/sequence-labelling.html"><strong aria-hidden="true">2.3.</strong> Sequence Labelling</a></li><li class="chapter-item expanded "><a href="../classical/grammars.html"><strong aria-hidden="true">2.4.</strong> Grammars</a></li></ol></li><li class="chapter-item expanded "><a href="../neural/neural.html"><strong aria-hidden="true">3.</strong> Neural</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../neural/rnn.html" class="active"><strong aria-hidden="true">3.1.</strong> RNN</a></li><li class="chapter-item expanded "><a href="../neural/transformers.html"><strong aria-hidden="true">3.2.</strong> Transformers</a></li><li class="chapter-item expanded "><a href="../neural/information-extraction.html"><strong aria-hidden="true">3.3.</strong> Information Extraction</a></li><li class="chapter-item expanded "><a href="../neural/machine-translation.html"><strong aria-hidden="true">3.4.</strong> Machine Translation</a></li><li class="chapter-item expanded "><a href="../neural/semantic-role-labelling.html"><strong aria-hidden="true">3.5.</strong> Semantic Role Labelling</a></li><li class="chapter-item expanded "><a href="../neural/word-sense-disambiguation.html"><strong aria-hidden="true">3.6.</strong> Word Sense Disambiguation</a></li><li class="chapter-item expanded "><a href="../neural/question-answering.html"><strong aria-hidden="true">3.7.</strong> Question Answering</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Little Book of NLP</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/jackdarlison/the-little-book-of-nlp" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="recurrent-neural-networks-rnns"><a class="header" href="#recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</a></h1>
<p>Language is inherently temporal in nature, meaning we want to be able to process words in relation to words which come before it and potentially after. </p>
<p>Simple recurrent networks (Elman Networks) achieve temporal processing by introducing recurrent connections. We model each input in the sequence as happening at a time \( t \) and the recurrent connections allow us to use the output of the hidden layer at \( t-1 \) as another input to the hidden layer at \( t \). Therefore information from previous inputs is aggregated and passed into future processing. </p>
<p>The complexity of the recurrent connections is reduced by unravelling the connections through time, creating a hidden layer of individual nodes which process the input \( t \) and previous hidden layer \( t-1 \). Training is then done similar to standard neural networks, propagating the loss through the time states and back-propagating to the start, altering the weights. </p>
<p>As recurrent networks rely on processing previous hidden states to calculate the current state, they can not be parallelised making large networks slow to compute. </p>
<h2 id="long-short-term-memory-lstm-cells"><a class="header" href="#long-short-term-memory-lstm-cells">Long-Short Term Memory (LSTM) cells</a></h2>
<p>LSTM cells are a more sophisticated form of recurrent connections, designed to give RNNs to ability to learn and forget information as well as solve the limited information retention that simple recurrent networks have. </p>
<p>This is achieved by the addition of the context output, which is also passed through time like the hidden layer outputs. </p>
<p><img src="../images/lstm.png" alt="LSTM" /></p>
<p>Information is added and removed from the context and hidden vectors through the use of gates, which consist of sigmoid activations and element-wise multiplication. </p>
<p>As LSTMs are modular in nature, so they can be unrolled through time like simple RNNs, just with the addition of a new input and output.</p>
<p>LSTMs (and its simpler version the Gated Recurrent Unit,  GRU) are the standard architecture for any recurrent based network</p>
<h2 id="rnn-architecture"><a class="header" href="#rnn-architecture">RNN Architecture</a></h2>
<h4 id="stacked-rnns"><a class="header" href="#stacked-rnns">Stacked RNNs</a></h4>
<p>As RNNs can output at each hidden state we can stack multiple layers of RNNs on top of each other, using the output from one layer as the input in the layer above. </p>
<p>This allows us to capture varying levels of abstraction across the levels, similar to how human perception works. Generally outperforming single layered networks</p>
<p>However, training costs rise quickly with the number of layers</p>
<h4 id="bidirectional-rnns"><a class="header" href="#bidirectional-rnns">Bidirectional RNNs</a></h4>
<p>Many applications have access to the whole input instead of just the preceding input. In these it would make sense to also use the right-to-left context to aid in predicting. In addition one directional RNNs naturally reflect more information about the end of a sentence than the beginning, a problem bidirectional solves. </p>
<p>This is achieved by training to RNNs, one processes the sequence forward and one processes it in reverse. The final hidden layer (or pooled outputs) are then combined. </p>
<p>Concatenation is the most common form of combining the two outputs, however, other methods such as element-wise addition or multiplication are acceptable. </p>
<h2 id="recurrent-network-patterns"><a class="header" href="#recurrent-network-patterns">Recurrent Network Patterns</a></h2>
<p>Recurrent networks can be assembled in multiple different ways, each searching a specific task. </p>
<h4 id="for-language-models"><a class="header" href="#for-language-models">For Language models</a></h4>
<p><img src="../images/language-modelling.png" alt="Language Modelling" /></p>
<p>To use RNNs for language models, we first input a start of sequence token (and some context) where the output (softmaxed) is then the predicted next word. At the next time step the previous prediction is used as the input, where this process repeats until a sequence length is reached or a end of sequence token is generated. </p>
<p>This process of using the previous output as the next input is known as <strong>autoregressive generation</strong></p>
<p>To train, teacher forcing is used which uses the correct input instead of the previous predicted word. This stops the model drifting to far from what is expected. To adjust the weights averaged cross entropy loss over the predicted words is used. </p>
<p><em>Weight tying</em> can reduce the number of parameters by using the same embedding matrix for the input for the output weights when predicting the next word.</p>
<h4 id="for-sequence-labelling"><a class="header" href="#for-sequence-labelling">For Sequence Labelling</a></h4>
<p><img src="../images/sequence-labelling.png" alt="Sequence labelling" /></p>
<p>For sequence labelling, the output of each hidden layer is used to predict the label for each input token.</p>
<h4 id="for-sequence-classification"><a class="header" href="#for-sequence-classification">For Sequence classification</a></h4>
<p><img src="../images/sequence-classification.png" alt="Sequence Classification" /></p>
<p>For sequence classification, all but the last hidden layers are ignored. The last hidden layer is fed into a separate MLP+softmax classifier. </p>
<p>The cross entropy loss from the classifier is used to train the whole network, known as end-to-end training</p>
<p>Instead of using the last hidden state we can use a pooling function to aggregate all the hidden states such as:</p>
<ul>
<li>element-wise mean</li>
<li>element-wise max</li>
<li>element-wise addition</li>
</ul>
<h4 id="for-sequence-to-sequence"><a class="header" href="#for-sequence-to-sequence">For Sequence-to-Sequence</a></h4>
<p><img src="../images/encoder-decoder.png" alt="Encoder-Decoder" /></p>
<p>Sequence to sequence (lengths are each may be different) problems such as machine translation combine two RNNs. The first network, the <strong>encoder</strong>, encodes the input sequence into a single vector known as the context vector, which represents all the information the second RNN needs to know about the first. The second network, the <strong>decoder</strong>, combines the context vector to generate the output sequence</p>
<p>The <em>context vector</em> can be created either by using the final hidden state or by pooling (max, mean, add, ...) all the hidden states of the encoder. </p>
<p>In addition the context vector is usually given to all states of the decoder directly, to ensure that its influence remains present thought out the entire decoder sequence. </p>
<p>Training the network is done end-to-end, using the averaged cross entropy loss of the decoder outputs to adjust the weights. Teacher forcing is also used in the decoder. </p>
<h2 id="attention"><a class="header" href="#attention">Attention</a></h2>
<p>In encoder-decoder models the context vector acts as a bottleneck, as it needs to represent everything there is to know about the input sequence in a single vector. </p>
<p>Attention solves this problem by taking a weighted sum over the hidden states of the encoder as each time step in the decoder. </p>
<p>Attention follows a three step process:</p>
<ul>
<li>Compute the relevance (score) of the previous decoder hidden state and each of the encoder hidden states.</li>
<li>Softmax the scores to create a probability distribution</li>
<li>Calculate the context vector for the current decoder state by using the distribution to take a weighted average over all the encoder hidden states. </li>
</ul>
<p>The most simple scoring function is the dot product between the two vectors (known as dot product attention), however it is also possible to use its own set of weights to score the two vectors which are trained with the rest of the model</p>
<p><strong>Cross attention</strong> is a more sophisticated form of attention, that is similar in nature to multi head <a href="transformers.html">Self Attention</a>, but the queries come from the decoder and the keys and values come from the encoder. </p>
<h2 id="beam-search"><a class="header" href="#beam-search">Beam Search</a></h2>
<p>So far in language models, those that generate the next probable word, the most likely word from the softmax distribution has been chosen. This method is inherently greedy, picking the best word at each state rather than the best word for the whole sequence at this point. </p>
<p>Exhaustively searching all possibilities is not feasible, so instead <strong>Beam search</strong> is used as a trade off between search space and computation. </p>
<p>At each output step beam search keeps track of the \( k \) best sequences. These are calculated by the joint probability of all the word probabilities in them. At each output the next word is calculated from the \( k \) sequences and the new \( k \) are chosen, ensuring the search width is never more than \( k \).</p>
<p>A sequence is removed when one of the best ends with a end of sequence token, it is saved for as a potential output sequence and 1 is subtracted from \( k \). Once all \( k \) sequences are found we can either pass all of them, a subset, or the best downstream</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../neural/neural.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../neural/transformers.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../neural/neural.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../neural/transformers.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
