<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Transformers - The Little Book of NLP</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="../../index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">The Little Book of NLP</li><li class="chapter-item expanded "><a href="../generic/generic.html"><strong aria-hidden="true">1.</strong> Generic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../generic/word-representations.html"><strong aria-hidden="true">1.1.</strong> Word Representations</a></li><li class="chapter-item expanded "><a href="../generic/metrics.html"><strong aria-hidden="true">1.2.</strong> Metrics</a></li><li class="chapter-item expanded "><a href="../generic/vector-similarity.html"><strong aria-hidden="true">1.3.</strong> Vector Similarity</a></li></ol></li><li class="chapter-item expanded "><a href="../classical/classical.html"><strong aria-hidden="true">2.</strong> Classical</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../classical/n-grams.html"><strong aria-hidden="true">2.1.</strong> N-Grams</a></li><li class="chapter-item expanded "><a href="../classical/text-processing.html"><strong aria-hidden="true">2.2.</strong> Text Processing</a></li><li class="chapter-item expanded "><a href="../classical/sequence-labelling.html"><strong aria-hidden="true">2.3.</strong> Sequence Labelling</a></li><li class="chapter-item expanded "><a href="../classical/grammars.html"><strong aria-hidden="true">2.4.</strong> Grammars</a></li></ol></li><li class="chapter-item expanded "><a href="../neural/neural.html"><strong aria-hidden="true">3.</strong> Neural</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../neural/rnn.html"><strong aria-hidden="true">3.1.</strong> RNN</a></li><li class="chapter-item expanded "><a href="../neural/transformers.html" class="active"><strong aria-hidden="true">3.2.</strong> Transformers</a></li><li class="chapter-item expanded "><a href="../neural/information-extraction.html"><strong aria-hidden="true">3.3.</strong> Information Extraction</a></li><li class="chapter-item expanded "><a href="../neural/machine-translation.html"><strong aria-hidden="true">3.4.</strong> Machine Translation</a></li><li class="chapter-item expanded "><a href="../neural/semantic-role-labelling.html"><strong aria-hidden="true">3.5.</strong> Semantic Role Labelling</a></li><li class="chapter-item expanded "><a href="../neural/word-sense-disambiguation.html"><strong aria-hidden="true">3.6.</strong> Word Sense Disambiguation</a></li><li class="chapter-item expanded "><a href="../neural/question-answering.html"><strong aria-hidden="true">3.7.</strong> Question Answering</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The Little Book of NLP</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/jackdarlison/the-little-book-of-nlp" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="transformers"><a class="header" href="#transformers">Transformers</a></h1>
<p>LSTMs and GRUs can handle long distance information, however, as they are recurrent in nature they cannot be parallelised, making them scale poorly with long sequence. </p>
<p>Transformers are an architecture that can handle both long distance information and be parallelised, making them the best model for language processing.</p>
<p>In a transformer each input has access to all previous inputs, known as causal transformers (variations exist using different combinations of inputs), allowing the model to handle distance information. Most importantly, calculations performed for the current input input are independent of other inputs, making the system able to be parallelised. </p>
<p>The core idea behind using other inputs to calculate the current input is the <strong>self-attention</strong>, which compares the current input to other inputs revealing their relevance in the current context. </p>
<p>Transformers are contained in a modular block, taking an input sequence and outputting a sequence of the same length allowing them to be stacked. The transformer block consists of 4 layers:</p>
<ol>
<li>Self-attention layer</li>
<li>Layer normalise</li>
<li>Feedforward layer</li>
<li>Layer normalise</li>
</ol>
<p>Each layer feeds into the next layer. Residual connections are also added around the self-attention and feedforward layers (input is added to the output vector), which allows higher layers direct access to the information from lower layers. The normalisation keeps the values of the layer outputs and residual connections in a range that facilitates gradient based learning. </p>
<p>Layer normalisation is implemented by subtracting the mean and dividing by the standard deviation, making a distribution with a mean of 0 and a standard deviation of 1. Gain and offset parameters are added after</p>
<h2 id="self-attention"><a class="header" href="#self-attention">Self-attention</a></h2>
<p>Self-attention is a more sophisticated variation of the attention used within encoder-decoder based models</p>
<p>In self-attention each input will play three different roles during the self-attention process:</p>
<ul>
<li><strong>Query</strong>: the current focus (i.e. input) of attention being compared to the other inputs</li>
<li><strong>Key</strong>: an input being compared to the current focus</li>
<li><strong>Value</strong>: the value used to compute the output for the current focus of attention</li>
</ul>
<p>To represent the roles a weight matrix will be created for each, which will be used to project the input \( x_i \) onto its representation of each role. \[q_i = W^Qx_i;\ k_i = W^Kx_i;\ v_i = W^Vx_i \]</p>
<p>Similar to encoder-decoder attention we need to calculate a relevance score for the current focus input \( x_i \) and each context input \( x_j \). However the scores are normalised by the dimensionality of the key (and query) vectors to avoid numerical issues</p>
<p>\[ score(x_i, x_j) = \frac{q_i \cdot k_j}{\sqrt{d_k}} \]</p>
<p>This is then softmax normalised and multiplied by \( v_i \)</p>
<p>Although, a each output can be computed independently there is no need to process each pair of inputs individually. We can instead represent the input as a matrix, and create matrices of the query, key, and value representations. This allows us to use efficient matrix multiplication implementations instead! </p>
<p>\[ Q = XW^Q;\ K=XW^K;\ V=XW^V \]</p>
<p>Given these matrices we can neatly represent the process of self attention as a single function:</p>
<p>\[ \text{SelfAttention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V \]</p>
<p>As all inputs are being used as keys for all queries, if only preceding inputs should be used then the top-right triangle of the self-attention matrix should be set to negative infinity (which will become 0 with softmax)</p>
<h4 id="multi-head-attention"><a class="header" href="#multi-head-attention">Multi-head Attention</a></h4>
<p>Different words in a sentence can relate to each other is in different ways simultaneously; it is difficult for a single self-attention layer to capture all these meanings. Multi-head attention solves this by using multiple parallel self-attentions layers at the same depth.</p>
<p>Each self attention layer is given its own set of query, key, and value matrices and undergoes self-attention in the same way. The output of each self-attention is then concatenated together and projected down to the original input dimension using another set of weights \( W^O \)</p>
<h4 id="positional-embeddings"><a class="header" href="#positional-embeddings">Positional Embeddings</a></h4>
<p>So far transformers have had no notion of where each input is within the input sequence, unlike RNNs where this is built into the recurrent structure of the network. </p>
<p>To achieve positional information we create a positional embedding for each word. This is achieved through  static functions which map integers to real values vectors such they retain positional relationships, the original transformer paper used a mixture of sine and cosine functions. This positional vectors share the same dimensions as the word embeddings and they are combined through addition. </p>
<h4 id="transformers-as-language-models"><a class="header" href="#transformers-as-language-models">Transformers as Language Models</a></h4>
<p>As autoregressive generation requires the calculation of the previous input, using transformers to generate text is not able to be made parallel. </p>
<p>It is still possible to train the transformers in parallel as using teacher forcing allows all calculations to be independent. We can then use averaged cross entropy loss over the predicted outputs to adjust the weights. </p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../neural/rnn.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../neural/information-extraction.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../neural/rnn.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../neural/information-extraction.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
